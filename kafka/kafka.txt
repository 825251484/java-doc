kafka高吞吐的实现：
1、顺序读写：
kafka的特点就是高性能，高吞吐，低延迟，吞吐量能达到几十万甚至几百万（增加分区，添加机器）
顺序读写：Kafka将消息写⼊到了分区partition中，⽽分区中消息是顺序读写的。磁盘顺序读写性能要远⾼于内存的随机读写。
2、零拷贝：零拷⻉并不是不需要拷⻉，⽽是减少不必要的cpu拷⻉次数。通常是说在IO读写过程中。Kafka利⽤linux操作系统的 "零拷⻉（zero-copy）" 机制在消费端做的优化。
	read：DMA付责将磁盘上的数据拷贝到内核空间。然后cpu付责将数据从内核空间拷贝到用户空间，用户才可以读取到数据
	write：read读取的数据在用户空间。cpu付责将用户空间的数据拷贝到内核空间的socket缓冲区，然后DMA付责将socket缓冲区的数据拷贝到网卡中。
	因为CPU在内核和用户空间的上下文切换，以及cpu拷贝，大大的降低了程序的执行性能。
	所以诞生了零拷贝技术（消费者）：sendfile + mmap。通过内存映射技术mmap，将磁盘数据映射到内核缓冲区的page cache，再使用sendfile函数通过dma将page cache中数据拷贝到网卡。
3、批量发送：Kafka允许使⽤批量消息发送模式
4、消息压缩：Kafka⽀持对消息集合进⾏压缩
5、Page Cache
kafka利用操作系统的内存，而非jvm内存，避免了内存溢出和GC的问题

顺序写和随机写：
磁盘分为顺序读写
与随机读写，内存也⼀样分为顺序读写与随机读写。基于磁盘的随机读写确实很慢，但磁盘的顺序读
写性能却很⾼，⼀般⽽⾔要⾼出磁盘随机读写三个数量级，⼀些情况下磁盘顺序读写性能甚⾄要⾼于
内存随机读写。
 磁盘的顺序读写是磁盘使⽤模式中最有规律的，并且操作系统也对这种模式做了⼤量优化，Kafka
就是使⽤了磁盘顺序读写来提升的性能。Kafka的message是不断追加到本地磁盘⽂件末尾的，⽽不
是随机的写⼊，这使得Kafka写⼊吞吐量得到了显著提升


⚫mmap（内存映射文件）
mmap 是一种系统调用，它将文件直接映射到进程的虚拟内存空间（堆外内存，由 操作系统 控制），使得文件读写像操作内存一样简单。这样当生产者写入时用户进程直接操作这块虚拟内存空间即可，读取也是
1）原理：在进程内创建一块虚拟内存，并标记该区域与某个文件关联，此时并没有实际加载文件数据到物理内存，只是建立了虚拟内存到文件的映射关系，
当进程访问这段虚拟内存时，会触发 缺页异常（Page Fault），操作系统才会真正从磁盘加载数据到pagecache中，然后将虚拟内存和pagechache做关联，当访问数据的时候，直接通过进程中的虚拟内存直接映射到pagechache。
如果pagecache无数据，在从磁盘io读取到pagecache
2）定义：Kafka 使用 mmap 技术将 磁盘上的日志文件（log segments）映射到进程的虚拟的内存空间（Page Cache），用户可以直接访问Page Cache中数据，不需要用户态切换到内核态，也不需要cpu拷贝，用户态程序直接读写Page Cache，无需调用read()/write()，因此避免了用户缓冲区的 CPU 拷贝。也避免用户态和内核态之间的数据拷贝。（避免cpu拷贝，不能避免dma拷贝，cpu资源比较宝贵，还避免用户态和内核态切换）
写入流程：Broker 写入消息（Producer → Broker），Producer 发送的消息先被追加到内存映射区域（Page Cache）。操作系统异步刷盘（fsync 由 log.flush.interval.messages 和 log.flush.interval.ms 控制）。
读取流程：消费者读取时，Broker 可以直接从Page Cache读取数据（如果数据仍在缓存中），避免磁盘 I/O。如果Page Cache没有数据，需要一次dma拷贝，将磁盘数据拷贝到page cache

⚫kafka与mmap
kafka中在消费者读取索引时用到了mmap。消费者访问索引以后查找log文件时，使用FileChannel 或 sendfile 。
kafka写入使用的是FileChannel.write() + PageCache 更高效
Broker 启动加载索引也使用了mmap
为什么这样设计？
1、进程内的虚拟内存地址空间有限不适合大文件的映射，所以适合索引文件，频繁读取且文件小。mmap映射的虚拟内存空间的大小，是和文件的大小一致的。也就是说文件10g，虚拟内存同样开辟一个10g的空间，即使实际只用了一小部分，但是同样会占用很多内存，尤其在 32 位系统易崩溃
2、严格顺序追加：Kafka 的日志写入是纯追加（Append-Only），FileChannel.write() 配合 PageCache 已能极致优化：顺序 I/O 的磁盘吞吐量可达 500MB/s 以上（HDD）或 几 GB/s（SSD/NVMe）。mmap 的随机写入优势在此场景无意义。


⚫sendfile 零拷贝——用于网络传输
Broker 发送消息给Consumer（Broker → Consumer）当消费者拉取消息时，Broker需要从磁盘（或Page Cache）读取数据并发送到网络。
传统方式：共 2次dma拷贝 + cpu拷贝 + 2 次上下文切换。
read(file, user_buf)：数据从磁盘 → dma → 内核缓冲区 → cpu → 用户缓冲区（拷贝 1）。
write(socket, user_buf)：数据从用户缓冲区 → cpu → Socket缓冲区 → dma → 网卡（拷贝 2）。
sendfile 优化：sendfile(file, socket)：数据直接从 Page Cache → 网卡缓冲区（DMA 传输），无需经过用户态。仅 1 次 DMA 拷贝 + 0 次用户态参与。


⚫Kafka 如何结合 mmap 和 sendfile？
写入阶段（Producer → Broker）：使用 mmap 将消息追加到日志文件（顺序写入 Page Cache）。依赖 OS 异步刷盘（除非 acks=all 需要同步刷盘）。
读取阶段（Broker → Consumer）如果数据在 Page Cache，直接使用 sendfile 发送到网络（零拷贝）。如果数据不在缓存，先mmap加载到Page Cache，再用sendfile发送。
sendfile 仅适用于 从文件到 Socket 的传输，而 mmap 用于 文件读写，两者互补而非替代
数据流向是 网络 → Broker → 磁盘，而 sendfile 仅支持 文件 → 网络。
⚫总结
mmap：优化 Kafka 磁盘存储，让 Broker 高效读写日志文件（顺序 I/O + Page Cache）。
sendfile：优化 Kafka 网络传输，让 Broker 向 Consumer 发送数据时避免用户态拷贝（Zero-Copy）。
两者结合：mmap 让数据快速进入 Page Cache，sendfile 让数据直接从 Page Cache 发到网络，共同实现 Kafka 的高吞吐、低延迟。
如果你的问题是面试题，可以回答："Kafka 在存储层使用 mmap 加速磁盘文件读写，在网络层使用 sendfile 实现零拷贝传输，两者共同保障高吞吐。"


⚫kafka零拷贝原理
原始拷贝流程：
	消费者发起请求，到kafka borker端，然后用户空间向cpu发起IO请求，发起read()系统调用，读取硬盘上的数据，这时上下文就会从用户态切换到内核态，read操作时阻塞操作，直到有数据返回，
	同时cpu对dma发起一个调度命令，通过dma控制器对硬盘发起一个io请求，然后在内存中找到需要消费的消息内容（参考下题），将数据拷贝到内核缓存区pagecache中（cpu不参与这次拷贝），拷贝结束后
	DMA会收到来至硬盘读取完毕的通知。dma收到信号，会告诉cpu，pagecache中数据准备好了。然后cpu将数据从内核缓存区拷贝到用户缓存区，read方法执行完成，，内核态切换用户态，被阻塞的用户进程会被唤醒
	，然后cpu将发起io操作wirte()命令（wirte方法阻塞直到返回），同时切换到内核态，cpu将数据拷贝到socket缓存区。然后cpu通知dma将数据拷贝到网卡设备，拷贝结束之后，会从内核态切换到用户态，
 	write阻塞结束。
	共经历：2次cpu拷贝，2次dma拷贝，4次内核用户态切换。
kafka拷贝：
	传统拷贝：cpu拷贝 + dma gather拷贝
		首先把read()调用改成了mmap()调用，同样需要用户态切换到内核态，mmap会建立一个地址映射区域mmap file，dma将找到的索引数据拷贝到内核空间，
		这时通过mmap技术用户直接访问索引，通过索引，找到log文件中的消息和附近的消息，将消息通过dma拷贝到内核空间，结束之后，线程从内核态切换到用户态，
		用户就可以读取共享区域的索引文件了，不需要cpu拷贝到用户空间了，然后cpu将内存中共享区域的地址和偏移量拷贝到socket缓存区。
		然后linus2.4诞生的dma gather技术，根据socket缓存区中的拷贝内容，找到对应的消息，将消息拷贝到网卡设备，传输到消费者进行消费。内核态切换到用户态
		共经历：dma拷贝，cpu地址拷贝（只拷贝地址速度很快），dma gather拷贝，2次内核用户态切换.
		用到技术：mmap内存映射，dma gather拷贝，顺序读，pagecache读，多种技术结合

	零拷贝：sendfile + dma gather拷贝（kafka最终使用的底层函数）
		和上一个比，改动点：
		【1】把mmap调用改成sendfile调用，
		【2】cpu地址拷贝改成了用非cpu的拷贝，那么就省去了cpu的开销，进一步提高性能，这里主要时linus内核的改进，省去了cpu拷贝
		共经历：dma拷贝，dma gather拷贝，2次内核用户态切换.
		用到技术：sendfile，dma gather拷贝，顺序读，pagecache读，多种技术结合

	splice + dma gather拷贝（kafka所有版本均未使用该函数）
		【1】把sendfile调用改成splice调用，
		【2】splice函数在pagecache和socket缓存区中间建立一个环形读写管道，写端绑定在pagecache，读端绑定到socket。数据从管道被读到socket中，然后将数据通过普通dma拷贝到网卡。

链接：https://www.bilibili.com/video/BV1iX4y1S76o/?spm_id_from=333.788&vd_source=80c3ca1a77e037f4d86501ba97d54dbb


⚫Broker 消息存储机制
分区日志：每个 Topic 的分区对应一个物理目录，目录名为 <topic>-<partition>（如 orders-0）。分区数据被拆分为多个段文件（Segment），默认每个段文件大小上限为 1GB（通过 log.segment.bytes 配置）。
段文件结构：每个段文件包含三个核心文件：文件均已偏移量命名
	.log 文件：存储实际消息，按顺序追加写入。
	.index 文件：存储消息的偏移量索引（稀疏索引），记录部分消息的物理位置（如每隔 4KB 建一条索引，通过 log.index.interval.bytes 配置）。
	.timeindex 文件（可选）：基于时间戳的索引，用于按时间范围查找。
文件命名规则：段文件以当前段的第一条消息的偏移量命名（如 00000000000012345678.log），方便快速定位。（从0开始，如果文件写满了，生成第二个文件以上一个文件最大的offset+1来命名）
写入流程：新消息追加到当前活跃段（Active Segment），写满后滚动生成新段。写入操作是顺序 I/O，性能极高。


⚫kafka如何消费消息
1、偏移量定位：确定目标 Offset 属于哪个 Segment（通过 .log 文件名定位）
2、根据目标消息的offset，通过二分查找对应段的索引文件，目标offset和index文件的名称做比较，判断在哪个索引文件，从索引文件中找到小于等于且最接近目标索引的索引，索引对应一个物理地址
3、物理位置 指的是 字节偏移量（Byte Offset），表示目标消息在 .log 文件中的起始位置（从文件开头计算的字节数）。
4、通过物理地址可以快速跳到对应的log文件，从该物理地址按顺序开始搜索，直到找到目标消息，通过零拷贝发给消费者
如果定位在00000000000001234000.index索引文件，那么消息肯定也在相同命名的log文件：00000000000001234000.log。所以通过文件的其实偏移量+物理地址，快速跳到log文件


⚫kafka的offset如何管理
定义：将 offset 存储在 Kafka 内部主题 __consumer_offsets 中，由 Broker 自动管理。50个分区会均匀的分配到多台broker集群中
内部主题：__consumer_offsets 是一个紧凑的（compact）Kafka 主题，默认有 50 个分区，存储所有消费者组的 offset 信息。可通过 offsets.topic.num.partitions 调整
自动提交：消费者默认每隔 auto.commit.interval.ms（默认 5 秒）自动提交一次 offset（需启用 enable.auto.commit=true）。
手动提交：可通过 consumer.commitSync() 或 consumer.commitAsync() 手动提交，确保消息处理完成后再提交 offset，避免丢失或重复消费。
当消费者提交 offset 时：
1、Kafka 根据 group.id 的哈希值 计算目标分区：targetPartition = hash(group.id) % 50  // 确定写入哪个分区（0~49）
2、写入请求发送到该分区的 Leader Broker。
3、Leader 将数据同步到所有 Follower，确保副本一致性（acks=all）。
读取 Offset（消费者视角）
1、消费者启动时：同样根据 group.id 哈希计算目标分区。
2、从该分区的 Leader Broker 读取 offset。


⚫__consumer_offsets结构
Key：由 group.id、主题名（topic）、分区号（partition）组成，唯一标识一个消费者组的消费进度。
Value：存储对应的 offset 值（以及元数据如提交时间戳）。
清理策略：该主题是 compact（压缩） 的，Kafka 会定期清理旧版本数据，仅保留每个 Key 的最新 Value。
例如：[group-1, test-topic, 0]  // 表示消费者组 group-1 对 test-topic 分区 0 的 offset


⚫预读机制
Kafka 不直接控制预读，而是依赖操作系统的 PageCache 机制。顺序读取时，操作系统会自动预读附近消息，显著提升后续读取性能。
预读大小：由操作系统内核参数控制（如 Linux 的 read_ahead_kb），默认可能预读 128KB~1MB 的数据。
优势：顺序读取 + 预读使得 Kafka 的消费者能以接近内存的速度消费数据。
劣势：如果生产者写入过快，可能挤占 PageCache，导致消费者读取时缓存命中率下降（可通过 vm.dirty_ratio 调优）。


⚫Kafka生产和消费数据之所以快的几个原因。
一：Sender()子线程用到了批量发送消息。
二：Sender()子线程在写数据的时候，是直接写到了pageCache中，相当于到目前为止，数据都是在内存中操作。
三：操作系统在将PageCache中的数据写到硬盘的时候，用到了顺序写。
四：消费者在消费数据的时候，一旦在PageCache中找不到想要消费的数据，使用到了“预读”操作，将数据预读到PageCache中，这样有极大的概率会用到顺序读。顺序读 最终目的还是为了减少磁盘读取的寻道概率
五：消费者在消费数据的时候，会使用“零拷贝”技术，将PageCache中的数据直接拷贝到网卡设备，进而实现快速消费。
六：使用PageCache，省去了对JVM的GC操作，从而避免发生Stop the World。

⚫kafka之所以快的原因？
简单来说，是因为他充分利用了操作系统cache，顺序写和零拷贝技术。可以从 存储、访问， 两个方面分析。
存储
1.数据的写入利用了操作系统提供的pageCache，属于堆外内存，降低java应用内存的管理压力，从而减少了gc带来的stop the word问题，基于内存操作，且采取顺序写入方式，效率极快。
2.底层对于数据存储，分为了数据，索引，关系映射三种类型文件，采用顺序分段存储，稀疏索引的方式式。提高数据查找的效率。
3.读取索引文件，底层通过mmap内存映射技术代替Filechannel.read()，无需cpu拷贝和内核态切换，用户态程序直接访问pagecache。将数据快速写入pagecache内存，然后操作系统异步刷盘
访问
3.mmap+sendfile  对于磁盘数据的读取，利用DMA（直接内存访问）的系统调用sendfile，由传统的 磁盘-内核缓冲-应用缓存-socket缓冲-网卡 文件copy方式，改进为 磁盘->pagecache->网卡，减少了 用户/内核态的转换次数和 文件copy次数。


⚫kafka消费分区策略
Kafka为什么建议使用CooperativeStickyAssignor（kafka2.4.0）消费分区策略，在发送Rebalance的时候，CooperativeStickyAssignor 相比 RangeAssignor、RoundRobinAssignor 和 StickyAssignor 这三者分区分配策略，不会发生全局的Stop the World
当发生rebalance时仍然能保证一部分的分区的消费
Group Coordinator渐进式的rebalance
例如：
消费者C1消费两个分区P0,P2
消费者C2消费一个分区P1
当有第三个消费者C3加入时，Group Coordinator会收到join Group通知，然后Group Coordinator会通知C1 C2进行rebalance，这两个消费者将自己持有的分区发送给Group Coordinator，
Group Coordinator经过计算，得出应该把P2分给C3，在整个rebalance这期间第一阶段就是所有分区正常消费，第二个阶段P2停止消费，第二阶段结束P2正常消费


⚫kafka生产者分区策略（发送消息出现延迟，如何调整）
生产者默认分区策略：RoundRobin
RoundRobin：先将消息缓存到RecordAccumulator内部batches中内存16k大小的ByteBuffer中，如果不设置key，消息将以轮询的方式发送不同的分区的batches中，每个batches填不满，（以消息纬度轮训，单条消息，比如6条消息，需要轮训6次，每次发送消息时，强制 切换到下一个分区）
Sticky：kafka2.4之后增加的粘性分区策略，只有批次填满或等待超时（linger.ms）才会切换到下一个分区，尽量让一个批次的所有消息进入同一个分区，减少网络请求和分区切换。（以batch.size纬度轮训，比如6条消息，3条消息填满一个batch.size或者超时才会切换分区，那就需要轮训2次）
区别：rr和sticky分区策略在数据延时上主要是因为在rr策略下，一个batch不能填满，导致相同一批数据rr策略会产生更多的batch，导致后续flush()的时候开销更大引起的
如果在发送消息特别快，然后分区数又很少（10个以内），linger.ms>100，消息的key非空，两者分区策略差异不是很大
Kafka ≤ 2.3：默认使用 RoundRobin（严格轮询）。
Kafka ≥ 2.4：默认使用 Sticky（优化吞吐量）。
参考链接；https://www.bilibili.com/video/BV1M8411G7Td/?spm_id_from=333.788&vd_source=80c3ca1a77e037f4d86501ba97d54dbb


⚫kafka生产者端RecordAccumulator
	RecordAccumulator内部结构
	    private final ConcurrentMap<TopicPartition, Deque<ProducerBatch>> batches;
	    private final BufferPool free;
 当生产者发送消息时，会把消息放在一个暂存区域batches中，batches中key为topic和partition，value就是存储ProducerBatch的队列，这里是批量发消息的关键。也是kafka高性能的原因之一。
 kafka会把多条消息都放在ProducerBatch中，打包一起发送，当ProducerBatch写满的时候，就会唤醒sender子线程，然后把数据发送到leader所在的broker中的pagecache上，如果设置了多副本机制
 那么从节点follower就会从leader中拉取消息。当所有节点拉完消息，（acks=10，retries=10），这个时候会向生产者反馈一个ack应答，告诉生产者已经写入成功。但是消息此时在pagecache中，
 没有写入到硬盘上。如果此时机器挂了，那么这些数据就会丢失，
 注意：
 生产者发送失败ack应答失败，生产者会重新发送。
 acks应答参数：
 0：我的KafkaProducer在客户端，只要把消息发送出去，不管那条数据有没有发送出去，哪怕Partition Leader没有写入磁盘，也不管他了，直接认为这个消息发送成功
 1：只要Partition Leader接收到消息且写入本地磁盘，就认为成功，不管他其他的Follower有没有同步过去这条消息
 all：Partition Leader接收到消息之后，还必须要求ISR列表里跟Leader保持同步的那些Follower都要把消息同步过去，才认为这条消息是写入成功的。失败会重新发送
 retries：重新发送的次数
 设置参数：
 min.insync.replicas=2： Kafka ISR 列表中最小同步副本数
 replication.factor=3，用来设置主题的副本数。每个主题可以有多个副本，副本位于集群中不同的broker上，也就是说副本的数量不能超过broker的数量，否则创建主题时会失败。
 链接：https://www.bilibili.com/video/BV1Ss4y1h7ea/?spm_id_from=333.788&vd_source=80c3ca1a77e037f4d86501ba97d54dbb


⚫kafka内存池化技术
Kafka通过使用内存缓冲池，让整个消息发送的过程中，内存空间可以循环利用，有效的降低了JVM GC的触发次数，从而提高了消息发送的速度，提升了吞吐量。
	RecordAccumulator内部结构
	    private final ConcurrentMap<TopicPartition, Deque<ProducerBatch>> batches;
	    private final BufferPool free;
	ProducerBatch内部结构：使用ByteBuffer来存储数据的。
	BufferPool：
		Deque<ByteBuffer> free 池化的空间
		nonPooledAvaliableMemory未被池化的可使用空间
	当生产者发送数据时，会根据消息体的大小向BufferPool申请一块内存16k大小的ByteBuffer，如果超过16k那么就向nonPooledAvaliableMemory申请一块内存空间来存放消息，
	因为对于磁化的空间中的内存是可以循环重复使用的，这块内存的使用和释放不需要等待GC来回收的，（就像线程池中的线程一样，一个道理，重复使用），
	没有GC的操作那么就不会发送STW，这也是kafka高性能，高吞吐的原因之一
⚫生产者阻塞（发送缓慢）原因：
	如果我当前发送的消息大小是20k，那么这是一个完整的消息，无法拆分，那么就需要申请池外空间20k大小来存储。也就是先判断16k池化空间够用，不够就申请池外空间，并且一个完整消息是不可拆分的
	非磁化内存使用的是堆内存，需要GC来处理释放内存，因此就比较消耗时间，发送消息就会比较缓慢，所以在生产中，我要尽量控制消息的大小，从而控制频繁的GC导致kafka的性能下降，（如果消息太大可以调整batch.size=16384）。
	解决：
	1.提高RecordAccumulator:buffer.memory设置合理范围值，池化空间，压测来决定 。 
	2.batch.size大小设置ByteBuffer大小，默认16k，max.request.size:1048576限制来单个请求中发送消息的大小，用这个数除以batch.size，得出一次发送请求中能发送多少个batch
	3.message.max.bytes:1000012限制borker端接收单条消息的大小，如果这个参数小于max.request.size会报错，因为发送比接收的大，所以要设置大于max.request.size
	4.fetch.max.bytes:5M（5242880） 消费者拉取消息的大小。
	建议：max.request.size < message.max.bytes < fetch.max.bytes
	
注意：
buffer.memory（默认32MB），这个参数代表的是RecordAccumulator所能容纳的最大的recordSize，其实这个参数经过层层传递，最终传给了BufferPool，由BufferPool来实际管理RecordAccumulator的容量。RecordAccumulator只需要关注自身的一些append、ready等等逻辑就好啦，职能很明确。
batch.size=16384设置ByteBuffer大小，一旦消息的大小达到这个值，就将消息发送给borker，
linger.ms默认0发送消息的延时时间，当达到这个时间，消息就会被发送到broker端，
compression.type消息压缩，减少占用的带宽（如果消费能力本身就不足，一般不建议压缩了，消息能力不足会消息积压，所以减少批量）
max.in.flight.requests.per.connection=5 指定了生产者在接收到服务器响应之前可以发送多个消息，值越高，占用的内存越大，当然也可以提升吞吐量，发生错误时，可能会造成数据的发送顺序改变，其默认值是5.
并且建议max.in.flight.requests.per.connection 的值小于 5。如何保证顺序，下面详解

⚫kafka会不会丢数据
即使我们把参数配置的很完善，也会丢失数据的两种场景：
一 当数据写到足够多的PageCache时，就会向生产者反馈acks响应，告知数据写入成功，但是如果此时，这些PageCache所在的操作系统挂了，如果数据还没写到硬盘上，这时数据就真的丢失了。
二 当副本数据所在的硬盘坏掉了，也会丢失数据。
注意：
我们可以改变以下参数来调整刷盘的频率，但是kafka官方不建议使用者调整。会影响kafka的性能，同时kafka官方认为可以用过副本机制来保证数据的可靠性
log.flush.interval.messages=10000 在持久化到磁盘前message最大接收条数。
log.flush.interval.ms=1000 任何主题中的消息在刷新到磁盘之前保留在内存中的最长时间（毫秒）。如果未设置，则使用 log.flush.scheduler.interval.ms 中的值
log.flush.scheduler.interval.ms 日志刷新程序检查是否需要将任何日志刷新到磁盘的频率（以毫秒为单位）


⚫如何保证数据不会丢失？
即使 acks=all，若 Broker未配置刷盘（flush），数据仍可能丢失（因为ISR副本可能也仅写入 Page Cache）。
完整可靠性需要 acks=all + min.insync.replicas=N（N≥2） + 同步刷盘（不推荐，性能差）。
min.insync.replicas：代表Kafka ISR 列表中最小同步副本数


⚫为什么 Kafka 默认不强制刷盘？
Kafka 认为分布式副本（Replication）是更高效的可靠性保障方式，而非依赖单机刷盘。
同步刷盘会使 Kafka 的吞吐量下降 1~2 个数量级。
Broker 崩溃的概率远低于网络或生产者故障，多副本机制已能覆盖大多数场景。

⚫kafka官网文档：https://kafka.apache.org/documentation.html#configuration
https://www.bilibili.com/video/BV1Ss4y1h7ea/?spm_id_from=333.788&vd_source=80c3ca1a77e037f4d86501ba97d54dbb

⚫如何尽量防止数据丢失呢？
将min.insync.replicas=2设置多副本，并设置log.dirs='多目录'：为了使每个目录对应一个独立的硬盘，防止硬盘坏了导致所有数据丢失


⚫kafka消息顺序及幂等性问题
Kafka本身支持 Exactly-Once语义：消息不丢失，消息不重复。因此实现消息发送的幂等关键是要实现 Broker端消息的去重。为了实现消息发送的幂等性，Kafka引入了两个新的概念：
【1】Producer ID (PID)	每个生产者启动时，Broker分配的唯一 ID，用于标识消息来源。
【2】Sequence Number (SN) 每个 <PID, Partition> 维护一个单调递增的序列号（从 0 开始）
【3】Broker 端缓存 记录每个 <PID, Partition> 已成功写入的最高序列号（如 lastSN=5）
Broker 会检查每条消息的 <PID, Partition, SN>：必须满足 SN == lastSN + 1，否则拒绝写入
例如：，3条消息，已写入 lastSN=0（消息1），下一条必须 SN=1，如果收到 SN=2（消息3），会直接拒绝并返回错误，
消息3（SN=2）到达 Broker：Broker 发现 lastSN=0，但 SN=2 != lastSN+1，因此 拒绝消息3，生产者收到错误后，会将消息3 重新加入发送队列，等待消息2重试成功
消息2（SN=1）重试成功：Broker 接受 SN=1（因 lastSN=0 + 1 = 1），更新 lastSN=1，之后，消息3（SN=2）再次尝试发送，此时 lastSN=1 + 1 = 2，成功写入
但是，只能保证单个 Producer对于同一个 <Topic, Partition>的 Exactly Once语义。不能保证同一个Producer一个 Topic不同的 Partion幂等。
如何保证有序性：
enable.idempotence=true；设置该参数，自动开启acks=all保证消息不丢 和 retries=Integer.MAX_VALUE无限重试，否则会丢弃消息，因为要遵守消息不丢失。
高吞吐+有序性：
enable.idempotence=true 和 max.in.flight.requests.per.connection = 10～20。会对在途的请求按 SeqNum 排序，能保证分区消息的有序性
代价：
1、会出现延迟
2、阻塞的问题：因为开启了幂等性和循序行，严格遵守顺序号，前序消息未到达，后续消息无法成功写入成功



⚫生产者发送失败的消息
当生产者发送消息失败时（如网络问题、Leader不可用等），Kafka生产者客户端会通过内置的重试机制（retries参数配置）将消息暂存到内存中的重试队列（本质是一个发送缓冲区），并延迟重试。这个队列是生产者内部的机制，并非Kafka服务端的持久化队列。
特点：
临时性：存在于生产者客户端内存中，进程重启后消失。
自动重试：达到重试次数后若仍失败，最终会抛出异常或触发回调（如onError）。
可控性：通过参数如retries、retry.backoff.ms配置。


⚫死信队列（Dead Letter Queue, DLQ）
死信队列是消息系统中通用的设计模式，用于存储无法被正常消费的消息（如格式错误、处理超时等）。
Kafka本身不原生支持DLQ，但可通过以下方式实现：消费者将处理失败的消息手动发送到另一个指定的Kafka Topic（即DLQ）。需要业务代码或框架（如Spring Kafka）配合实现。


⚫消息如何发送到kafka集群的broker节点中
https://www.bilibili.com/video/BV1584y1H7BP/?spm_id_from=333.788&vd_source=80c3ca1a77e037f4d86501ba97d54dbb
链接：https://blog.csdn.net/muyimo/article/details/118877254


⚫kafka leader选举
https://www.cnblogs.com/luedong/p/16023061.html


⚫kafka消息积压怎么处理？
1.多线程
2.创建更大的topic设置更多的分区和消费者，将原数据写入到新的topic中

⚫延时消息实现方式？
1.时间轮
2.内存队列+kafka，参考rocketMq
3.redis zset + kafka
4.rocksDB + kafka

⚫kafka broker如何参数调优？
	socket.receive.buffer.bytes：102400，单位是byte，socket接受缓冲区的大小，提高发送速度，一般根据网络传输速度和往返延迟时间来设置,如果使用操作系统的socket缓存区那就设置成-1
	socket.send.buffer.bytes：102400
	num.network.threads：3，处理网络请求的线程池大小，默认是3，接收生产者发送的消息请求。响应消费者拉取消息的请求。若 Broker 的 CPU 资源充足且网络流量大（如高吞吐场景），可适当增加此值（如 8~16）。
	queued.max.requests：500，请求队列中队列长度的大小，能放多少个请求，根据batch.size或等待linger.ms时间后，就会进入队列，sender子线程会从该队列获取就绪的消息，发送消息到borker，队列满了之后会被阻塞。
	queued.max.requests.byte：-1，请求队列容量的大小
	num.io.threads:8，磁盘io刷盘的线程，1、生产者的消息写入请求, 2、消费者的消息读取请求 3、副本之间的数据同步请求，这些线程运行在一个线程池中，采用"reactor模式"处理请求（io多路复用），默认=cpu，建议2*cpu，配合num.network.threads：处理网络连接的线程数(接收请求但不处理I/O)
	max.in.flight.requests.per.connection：高吞吐需求：需要最大化发送并行度，业务容忍乱序：分区内消息顺序不重要，消息重试导致无法保证顺序。
刷盘频率参数：时间和条数
	log.flush.interval.ms：null，多少时间把pagecache中刷到硬盘上，
	log.flush.interval.messages:Long.MAX_VALUE，累计多少条数据一起刷到硬盘上。 =1代表同步刷盘，此时就要增加num.io.threads:8参数，但一般刷盘都交给操作系统linux系统来刷盘
主从复制参数：
	num.replica.fetchers:1，对于大型集群或高吞吐量场景，可以增加到3-5，主从复制的线程数，如果设置acks=all那么就要提高该线程数，因为生产者发送消息，borker会将所有isr节点都同步之后再返回，如果设置1那么，ack响应就会很慢。，加快副本同步速度，减少副本滞后
	replica.fetch.min.bytes:1，每次复制时复制的字节数，，提高网络吞吐效率，减少小数据包传输。增加副本同步延迟，可能导致副本滞后增加，典型值范围：1024(1KB)到65536(64KB)
	replica.fetch.wait.max.ms:500：每次拉取等待的时间，要么达到上面的字节数在拉取复制，要么等待500ms拉去一次复制
面试回答：
高并发高吞吐的场景下如何优化
1、调整生产者分区策略stick，减少网络请求次数和频繁切换分区，批量发送
2、调整max.in.flight，增加并发度
3、适当增加batch.size


参考链接：https://www.bilibili.com/video/BV1FP4y167nH/?spm_id_from=333.788&vd_source=80c3ca1a77e037f4d86501ba97d54dbb

⚫pageCache污染
长尾消费导致的pageCache污染：已经被写到硬盘上的很久的数据。被消费，重新加载到pagecache中。

⚫kafka如何防止脑裂
在zk中存储一个controller_epoch值，这个值记录当前的控制器是第几代控制器，如果发生脑裂，那么会产生两个controller节点，新节点得epoch会+1，这时向新controller发生得请求会携带旧得epoch，小于新controller中得值，认为是无效得。

⚫kafka集群架构
Broker：Kafka集群中的每个服务器节点称为Broker，负责消息的存储和转发
Topic：消息的主题/类别，生产者向Topic发布消息，消费者从Topic订阅消息
Partition：每个Topic分为一个或多个Partition，实现消息的并行处理和水平扩展
Replica：每个Partition有多个副本（Replica），分布在不同的Broker上，保证高可用
Leader/Follower：每个Partition的多个副本中，一个为Leader，其他为Follower

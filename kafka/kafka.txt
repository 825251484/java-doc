kafka高吞吐的实现：
1、顺序读写：
kafka的特点就是高性能，高吞吐，低延迟，吞吐量能达到几十万甚至几百万（增加分区，添加机器）
顺序读写：Kafka将消息写⼊到了分区partition中，⽽分区中消息是顺序读写的。磁盘顺序读写性
能要远⾼于内存的随机读写。
2、零拷贝：零拷⻉并不是不需要拷⻉，⽽是减少不必要的拷⻉次数。通常是说在IO读写过程中。Kafka利⽤linux操作系统的 "零拷⻉（zero-copy）" 机制在消费端做的优化。
	read：DMA付责将磁盘上的数据拷贝到内核空间。然后cpu付责将数据从内核空间拷贝到用户空间，用户才可以读取到数据
	write：用户写入数据在用户空间。cpu付责将用户空间的数据拷贝到内核空间，然后DMA付责将内核空间的数据拷贝到网卡中。
	因为CPU在内核和用户空间的上下文切换，以及数据拷贝，大大的降低了程序的执行性能。
	所以诞生了零拷贝技术：mmap。通过设置一块内存共享区域。通过内存映射文件的机制。减少一次cpu拷贝。大大提高了性能
3、批量发送：Kafka允许使⽤批量消息发送模式
4、消息压缩：Kafka⽀持对消息集合进⾏压缩
5、Page Cache
kafka利用操作系统的内存，而非jvm内存，避免了内存溢出和GC的问题

顺序写和随机写：
磁盘分为顺序读写
与随机读写，内存也⼀样分为顺序读写与随机读写。基于磁盘的随机读写确实很慢，但磁盘的顺序读
写性能却很⾼，⼀般⽽⾔要⾼出磁盘随机读写三个数量级，⼀些情况下磁盘顺序读写性能甚⾄要⾼于
内存随机读写。
 磁盘的顺序读写是磁盘使⽤模式中最有规律的，并且操作系统也对这种模式做了⼤量优化，Kafka
就是使⽤了磁盘顺序读写来提升的性能。Kafka的message是不断追加到本地磁盘⽂件末尾的，⽽不
是随机的写⼊，这使得Kafka写⼊吞吐量得到了显著提升

⚫kafka零拷贝原理
原始拷贝流程：
	消费者发起请求，到kafka borker端，然后用户空间向cpu发起IO请求，发起read()系统调用，读取硬盘上的数据，这时上下文就会从用户态切换到内核态，read操作时阻塞操作，直到有数据返回，
	同时cpu对dma发起一个调度命令，通过dma控制器对硬盘发起一个io请求，然后在内存中找到需要消费的消息内容（参考下题），将数据拷贝到内核缓存区pagecache中（cpu不参与这次拷贝），拷贝结束后
	DMA会收到来至硬盘读取完毕的通知。dma收到信号，会告诉cpu，pagecache中数据准备好了。然后cpu将数据从内核缓存区拷贝到用户缓存区，read方法执行完成，，内核态切换用户态，被阻塞的用户进程会被唤醒
	，然后cpu将发起io操作wirte()命令（wirte方法阻塞直到返回），同时切换到内核态，cpu将数据拷贝到socket缓存区。然后cpu通知dma将数据拷贝到网卡设备，拷贝结束之后，会从内核态切换到用户态，
 	write阻塞结束。
	共经历：2次cpu拷贝，2次dma拷贝，4次内核用户态切换。
kafka零拷贝：
	mmap + dma gather拷贝
		首先把read()调用改成了mmap()调用，同样需要用户态切换到内核态，mmap会建立一个地址映射区域mmap file，dma将找到的索引数据拷贝到内核空间，
		这时通过mmap技术用户直接访问索引，通过索引，找到log文件中的消息和附近的消息，将消息通过dma拷贝到内核空间，结束之后，线程从内核态切换到用户态，
		用户就可以读取共享区域的索引文件了，不需要cpu拷贝到用户空间了，然后cpu将内存中共享区域的地址和偏移量拷贝到socket缓存区。
		然后linus2.4诞生的dma gather技术，根据socket缓存区中的拷贝内容，找到对应的消息，将消息拷贝到网卡设备，传输到消费者进行消费。内核态切换到用户态
		共经历：dma拷贝，cpu地址拷贝（只拷贝地址速度很快），dma gather拷贝，2次内核用户态切换.
		用到技术：mmap内存映射，dma gather拷贝，顺序读，pagecache读，多种技术结合

	sendfile + dma gather拷贝
		和上一个比，改动点：
		【1】把mmap调用改成sendfile调用，
		【2】cpu地址拷贝改成了用非cpu的拷贝，那么就省去了cpu的开销，进一步提高性能，这里主要时linus内核的改进，省去了cpu拷贝
		共经历：dma拷贝，dma gather拷贝，2次内核用户态切换.
		用到技术：sendfile，dma gather拷贝，顺序读，pagecache读，多种技术结合

	splice + dma gather拷贝
		【1】把sendfile调用改成splice调用，
		【2】splice函数在pagecache和socket缓存区中间建立一个环形读写管道，写端绑定在pagecache，读端绑定到socket。数据从管道被读到socket中，然后将数据通过普通dma拷贝到网卡。

链接：https://www.bilibili.com/video/BV1iX4y1S76o/?spm_id_from=333.788&vd_source=80c3ca1a77e037f4d86501ba97d54dbb

⚫kafka消费消息
kafka底层时按照分区存储数据的，每个分区下以文件大小存储消息，包含index索引和log消息文件，文件名用offset来命名，从0开始，如果文件写满了，生成第二个文件以上一个文件最大的offset+1来命名。
根据offset通过二分查找（跳表时间复杂度一样），找到对应的索引文件，小于等于自己的索引文件，通过找到索引文件中比小于等于自己的最大的相对偏移量，相对偏移量对应着log文件的物理地址，
通过这个物理地址去log文件中到对应的消息。找到对应的消息，就要将消息从磁盘拷贝到内核态中的pageCache中。
注意：地址附近的消息也会拷贝到pageCache中,预判断一些数据，下次读取直接从pageCache中读取，提高性能，吞吐量


⚫Kafka生产和消费数据之所以快的几个原因。
一：Sender()子线程用到了批量发送消息。
二：Sender()子线程在写数据的时候，是直接写到了pageCache中，相当于到目前为止，数据都是在内存中操作。
三：操作系统在将PageCache中的数据写到硬盘的时候，用到了顺序写。
四：消费者在消费数据的时候，一旦在PageCache中找不到想要消费的数据，使用到了“预读”操作，将数据预读到PageCache中，这样有极大的概率会用到顺序读。顺序读 最终目的还是为了减少磁盘读取的寻道概率
五：消费者在消费数据的时候，会使用“零拷贝”技术，将PageCache中的数据直接拷贝到网卡设备，进而实现快速消费。
六：使用PageCache，省去了对JVM的GC操作，从而避免发生Stop the World。

⚫kafka之所以快的原因？
简单来说，是因为他充分利用了操作系统cache，顺序写和零拷贝技术。可以从 存储、访问， 两个方面分析。
存储
1.数据的写入利用了操作系统提供的pageCache，属于堆外内存，降低java应用内存的管理压力，从而减少了gc带来的stop the word问题，基于内存操作，且采取顺序写入方式，效率极快。
2.底层对于数据存储，分为了数据，索引，关系映射三种类型文件，采用顺序分段存储，稀疏索引的方式式。提高数据查找的效率。
访问
3.dma+sendfile  对于磁盘数据的读取，利用DMA（直接内存访问）的系统调用sendfile，由传统的 磁盘-内核缓冲-应用缓存-socket缓冲-网卡 文件copy方式，改进为 磁盘-内核缓冲-socket缓冲-网卡，减少了 用户/内核态的转换次数和 文件copy次数。
4.dma+mmap ，利用DMA提供的mmap函数调用，允许内核态缓冲区直接引用文件句柄，与用户态缓冲区内存共享，从而实现像访问内存那样直接访问文件。减少文件copy次数和态转换。


⚫kafka消费分区策略
Kafka为什么建议使用CooperativeStickyAssignor（kafka2.4.0）消费分区策略，在发送Rebalance的时候，CooperativeStickyAssignor 相比 RangeAssignor、RoundRobinAssignor 和 StickyAssignor 这三者分区分配策略，不会发生全局的Stop the World
当发生rebalance时仍然能保证一部分的分区的消费
Group Coordinator渐进式的rebalance
例如：
消费者C1消费两个分区P0,P2
消费者C2消费一个分区P1
当有第三个消费者C3加入时，Group Coordinator会收到join Group通知，然后Group Coordinator会通知C1 C2进行rebalance，这两个消费者将自己持有的分区发送给Group Coordinator，
Group Coordinator经过计算，得出应该把P2分给C3，在整个rebalance这期间第一阶段就是所有分区正常消费，第二个阶段P2停止消费，第二阶段结束P2正常消费

⚫kafka的RecordAccumulator
	RecordAccumulator内部结构
	    private final ConcurrentMap<TopicPartition, Deque<ProducerBatch>> batches;
	    private final BufferPool free;
 当生产者发送消息时，会把消息放在一个暂存区域batches中，batches中key为topic和partition，value就是存储ProducerBatch的队列，这里时批量发消息的关键。也是kafka高性能的原因之一。
 kafka会把多条消息都放在ProducerBatch中，打包一起发送，当ProducerBatch写满的时候，就会唤醒sender子线程，然后把数据发送到leader所在的broker中的pagecache上，如果设置了多副本机制
 那么从节点follower就会从leader中拉取消息。当所有节点拉完消息，（acks=10，retries=10），这个时候会向生产者反馈一个ack应答，告诉生产者已经写入成功。但是消息此时在pagecache中，
 没有写入到硬盘上。如果此时机器挂了，那么这些数据就会丢失，
 注意：
 生产者发送失败ack应答失败，生产者会重新发送。
 acks应答参数：
 0：我的KafkaProducer在客户端，只要把消息发送出去，不管那条数据有没有发送出去，哪怕Partition Leader没有写入磁盘，也不管他了，直接认为这个消息发送成功
 1：只要Partition Leader接收到消息且写入本地磁盘，就认为成功，不管他其他的Follower有没有同步过去这条消息
 all：Partition Leader接收到消息之后，还必须要求ISR列表里跟Leader保持同步的那些Follower都要把消息同步过去，才认为这条消息是写入成功的。失败会重新发送
 retries：重新发送的次数
 设置参数：
 min.insync.replicas=2： Kafka ISR 列表中最小同步副本数
 replication.factor=3，用来设置主题的副本数。每个主题可以有多个副本，副本位于集群中不同的broker上，也就是说副本的数量不能超过broker的数量，否则创建主题时会失败。
 链接：https://www.bilibili.com/video/BV1Ss4y1h7ea/?spm_id_from=333.788&vd_source=80c3ca1a77e037f4d86501ba97d54dbb


⚫kafka会不会丢数据
即使我们把参数配置的很完善，也会丢失数据的两种场景：
一 当数据写到足够多的PageCache时，就会向生产者反馈acks响应，告知数据写入成功，但是如果此时，这些PageCache所在的操作系统挂了，如果数据还没写到硬盘上，这时数据就真的丢失了。
二 当副本数据所在的硬盘坏掉了，也会丢失数据。
注意：
我们可以改变以下参数来调整刷盘的频率，但是kafka官方不建议使用者调整。会影响kafka的性能，同时kafka官方认为可以用过副本机制来保证数据的可靠性
log.flush.interval.messages=10000 在持久化到磁盘前message最大接收条数。
log.flush.interval.ms=1000 任何主题中的消息在刷新到磁盘之前保留在内存中的最长时间（毫秒）。如果未设置，则使用 log.flush.scheduler.interval.ms 中的值
log.flush.scheduler.interval.ms 日志刷新程序检查是否需要将任何日志刷新到磁盘的频率（以毫秒为单位）

⚫kafka官网文档：https://kafka.apache.org/documentation.html#configuration
https://www.bilibili.com/video/BV1Ss4y1h7ea/?spm_id_from=333.788&vd_source=80c3ca1a77e037f4d86501ba97d54dbb

⚫如何尽量防止数据丢失呢？
将min.insync.replicas=2设置多副本，并设置log.dirs='多目录'：为了使每个目录对应一个独立的硬盘，防止硬盘坏了导致所有数据丢失

⚫kafka内存池化技术
Kafka通过使用内存缓冲池，让整个消息发送的过程中，内存空间可以循环利用，有效的降低了JVM GC的触发次数，从而提高了消息发送的速度，提升了吞吐量。
	RecordAccumulator内部结构
	    private final ConcurrentMap<TopicPartition, Deque<ProducerBatch>> batches;
	    private final BufferPool free;
	ProducerBatch内部结构：使用ByteBuffer来存储数据的。
	BufferPool：
		Deque<ByteBuffer> free 池化的空间
		nonPooledAvaliableMemory未被池化的可使用空间
	当生产者发送数据时，会根据消息体的大小向BufferPool申请一块内存16k大小的ByteBuffer，如果超过16k那么就向nonPooledAvaliableMemory申请一块内存空间来存放消息，
	因为对于磁化的空间中的内存是可以循环重复使用的，这块内存的使用和释放不需要等待GC来回收的，（就像线程池中的线程一样，一个道理，重复使用），
	没有GC的操作那么就不会发送STW，这也是kafka高性能，高吞吐的原因之一
	非磁化内存是需要GC来处理的，所以在生产中，我要尽量控制消息的大小，从而控制频繁的GC导致kafka的性能下降，（如果消息太大可以调整batch.size=16384）。
	
注意：
buffer.memory（默认32MB），这个参数代表的是RecordAccumulator所能容纳的最大的recordSize，其实这个参数经过层层传递，最终传给了BufferPool，由BufferPool来实际管理RecordAccumulator的容量。RecordAccumulator只需要关注自身的一些append、ready等等逻辑就好啦，职能很明确。
batch.size=16384设置ByteBuffer大小，一旦消息的大小达到这个值，就将消息发送给borker，
linger.ms默认0发送消息的延时时间，当达到这个时间，消息就会被发送到broker端，
compression.type消息压缩，减少占用的带宽（如果消费能力本身就不足，一般不建议压缩了，消息能力不足会消息积压，所以减少批量）
max.in.flight.requests.per.connection=5 指定了生产者在接收到服务器相应之前可以发送多个消息，值越高，占用的内存越大，当然也可以提升吞吐量，发生错误时，可能会造成数据的发送顺序改变，其默认值是5.
并且建议max.in.flight.requests.per.connection 的值小于 5。如何保证顺序，下面详解

⚫kafka消息顺序及幂等性问题
Kafka本身支持 At least once消息送达语义，因此实现消息发送的幂等关键是要实现 Broker端消息的去重。为了实现消息发送的幂等性，Kafka引入了两个新的概念：
【1】PID：每个新的 Producer在初始化的时候会被分配一个唯一的PID，这个 PID对用户是不可见的；
【2】Sequence Numbler：对于每个PID，该 Producer发送数据的每个<Topic, Partition>都对应一个从0开始单调递增的Sequence Number；
Broker端在内存中保存了这 Sequence Numbler，对于接收的每条消息，如果其序列号的值（SN_new）比 Broker端中维护的对应的序列号的值（SN_old）大1（即SN_new = SN_old + 1）时，Broker才会接收它，否则将其丢弃。这样就可以实现了消息重复提交了。但是，只能保证单个 Producer对于同一个 <Topic, Partition>的 Exactly Once语义。不能保证同一个Producer一个 Topic不同的 Partion幂等。
有序性：max.in.flight.requests.per.connection 小于等于 5，且开启幂等性enable.idempotence=true，broker 会对在途的请求按 SeqNum 排序，能保证分区消息的有序性


⚫消息如何发送到kafka集群的broker节点中
https://www.bilibili.com/video/BV1584y1H7BP/?spm_id_from=333.788&vd_source=80c3ca1a77e037f4d86501ba97d54dbb
链接：https://blog.csdn.net/muyimo/article/details/118877254


⚫kafka leader选举
https://www.cnblogs.com/luedong/p/16023061.html


⚫kafka消息积压怎么处理？

⚫kafka如何实现顺序消息？

⚫kafka broker如何参数调优？
